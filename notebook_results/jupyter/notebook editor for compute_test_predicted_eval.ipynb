{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "associatedRecipe": "compute_test_predicted_eval",
    "customFields": {},
    "tags": [
      "recipe-editor"
    ],
    "creator": "admin",
    "createdOn": 1737281839514,
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n\n# Read recipe inputs\ntest_predicted_merged \u003d dataiku.Dataset(\"test_predicted_merged\")\ntest_predicted_merged_df \u003d test_predicted_merged.get_dataframe()\n\n\n# Compute recipe outputs from inputs\n# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n\ndef evaluate_binary_classification(y_true, y_pred):\n  \"\"\"\n  Evaluates the performance of a binary classification model.\n\n  Args:\n    y_true: True labels.\n    y_pred: Predicted labels.\n\n  Returns:\n    A dictionary containing the following metrics:\n      - accuracy\n      - precision\n      - recall\n      - f1_score\n      - roc_auc_score\n      - confusion_matrix\n  \"\"\"\n\n  accuracy \u003d accuracy_score(y_true, y_pred)\n  precision \u003d precision_score(y_true, y_pred)\n  recall \u003d recall_score(y_true, y_pred)\n  f1 \u003d f1_score(y_true, y_pred)\n  roc_auc \u003d roc_auc_score(y_true, y_pred)\n  conf_matrix \u003d confusion_matrix(y_true, y_pred)\n  tn, fp, fn, tp \u003d conf_matrix.ravel() \n  return {\n      \u0027accuracy\u0027: accuracy,\n      \u0027precision\u0027: precision,\n      \u0027recall\u0027: recall,\n      \u0027f1_score\u0027: f1,\n      \u0027roc_auc\u0027: roc_auc,\n#     \u0027confusion_matrix\u0027: conf_matrix\n      \u0027True Negatives\u0027: [tn],\n      \u0027False Positives\u0027: [fp],\n      \u0027False Negatives\u0027: [fn],\n      \u0027True Positives\u0027: [tp]\n  }\n\n# \ny_true \u003d test_predicted_merged_df[\u0027fraud_bool\u0027]\ny_pred \u003d test_predicted_merged_df[\u0027prediction\u0027]\n\nmetrics \u003d evaluate_binary_classification(y_true, y_pred)\n\n#print(\"Accuracy:\", metrics[\u0027accuracy\u0027])\n#print(\"Precision:\", metrics[\u0027precision\u0027])\n#print(\"Recall:\", metrics[\u0027recall\u0027])\n#print(\"F1-score:\", metrics[\u0027f1_score\u0027])\n#print(\"ROC AUC:\", metrics[\u0027roc_auc\u0027])\n#print(\"Confusion Matrix:\\n\", metrics[\u0027confusion_matrix\u0027])\n\ntest_predicted_eval_df \u003d df \u003d pd.DataFrame.from_dict(metrics)  # For this sample code, simply copy input to output\n\n\n# Write recipe outputs\ntest_predicted_eval \u003d dataiku.Dataset(\"test_predicted_eval\")\ntest_predicted_eval.write_with_schema(test_predicted_eval_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "1 rows successfully written (xvdJSkQRsq)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(\"Confusion Matrix:\\n\", metrics[\u0027confusion_matrix\u0027])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Confusion Matrix:\n [[211572  82086]\n [   386   2956]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#metrics[\u0027confusion_matrix\u0027].ravel()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "array([211572,  82086,    386,   2956], dtype\u003dint64)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#(211572+2956)/(82086+386+211572+2956)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "0.7223164983164984"
          },
          "metadata": {}
        }
      ]
    }
  ]
}